Modern data centers use virtualization as a means to increase utilization of increasingly powerful multi-core servers. Applications often require only a fraction of the resources provided by modern hardware. Multiple concurrent workloads are therefore required to achieve adequate utilization levels. Current virtualization solutions allow hardware to be partitioned into Virtual Machines with appropriate isolation on most levels. However, unmanaged consolidation of resource intensive workloads can still lead to unexpected performance variance. Measures are required to avoid or reduce performance interference and provide predictable service levels for all applications. In this paper, we identify and reduce network-related interference effects using performance models based on the runtime characteristics of virtualized workloads. We increase the applicability of existing training data by adding network-related performance metrics and benchmarks. Using the extended set of training data, we predict performance degradation with existing modeling techniques as well as combinations thereof. Application clustering is used to identify several new network-related application types with clearly defined performance profiles. Finally, we validate the added value of the improved models by introducing new scheduling techniques and comparing them to previous efforts. We demonstrate how the inclusion of network-related parameters in performance models can significantly increase the performance of consolidated workloads.