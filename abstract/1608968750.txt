It is challenging to get reliable performance benchmarking results. Benchmarking matters because one of the defining characteristics of big data systems is the ability to process large datasets faster. “How large” and “how fast” drive technology choices, purchasing decisions, and cluster operations. Even with the best intentions, performance benchmarking is fraught with pitfalls-easy to get numbers, hard to tell if they are sound. This paper discusses five common pitfalls drawn from engineering and customer experiences at Cloudera, a leading big data vendor. These pitfalls are: “Comparing Apples to Oranges”-when too many parameters are modified and comparison is impossible, “Not Testing at Scale”-trying to test a big data system by extrapolating from an undersized test system, “Believing in Miracles”-failing to question suspicious results, “Using Unrealistic Benchmarks”-using workloads far removed from what will realistically be used by customers, and “Communicating Results Poorly”-neglecting to communicate sufficient information for customers to understand and reproduce the results. These pitfalls offers a behind-the-scenes look at internal engineering and review processes that produces rigorous benchmark results. Readers working on big data in both the industry and in academia can draw lessons from our experience.